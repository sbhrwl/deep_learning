base:
  project_name: MNIST Digit Analysis
  dataset_name: mnist datset

perceptron_config:
  artifacts_dir: artifacts/perceptron-model/AND_model.model

ann_mnist_config:
  artifacts_dir: artifacts\ann-mnist-model
  checkpoint_path: artifacts\ann-mnist-model\model_ckpt.h5
  tensorboard_logs: artifacts\tensorboard_logs
  learning_curve_plot: artifacts\ann-mnist-model\learning_curve_plot.png

mlflow_config:
  artifacts_dir: artifacts/mlflow-artifacts
  remote_server_uri: http://localhost:1234
  experiment_name: MNIST model
  tensorflow_auto_log_every_n_iter: 2
  # 2: mlflow will log the training metrics (loss, accuracy, and validation loss etc.) every 2 epochs

model_metrics:
    loss_function: "sparse_categorical_crossentropy"  # use => tf.losses.sparse_categorical_crossentropy
    optimizer: "adam"  # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
    metrics: [ "accuracy" ]

model_training_parameters:
  epochs: 2
  batch: 100
  configure_layers:
    hidden_layer_1_name: hidden_layer_1
    hidden_layer_1_activation: relu
    hidden_layer_1_number_of_neurons: 300
    hidden_layer_2_name: hidden_layer_2
    hidden_layer_2_activation: relu
    hidden_layer_2_number_of_neurons: 100
    output_layer_name: output_layer
    output_layer_activation: softmax
    output_layer_number_of_neurons: 10

model_transfer_learning:
  model_to_load: artifacts/ann-mnist-model/Model_2021_06_07_20_19_03_.h5
  model_metrics:
    loss_function: tf.losses.sparse_categorical_crossentropy
    optimizer: tf.keras.optimizers.SGD(learning_rate=1e-3)
    metrics: [ 'accuracy' ]

hyper_parameter_tuning:
  number_of_hidden_layers: [20, (40, 20), (45, 30, 15)]
  activation_functions: ['relu']# ['sigmoid', 'relu']
  batch_sizes: [50] # [128, 256]
  number_of_epochs: [2]
