base:
  project_name: MNIST Digit Analysis
  dataset_name: mnist datset

perceptron_config:
  artifacts_dir: artifacts/perceptron-model/AND_model.model

ann_mnist_config:
  artifacts_dir: artifacts\ann-mnist-model
  checkpoint_path: artifacts\ann-mnist-model\model_ckpt.h5
  tensorboard_logs: artifacts\tensorboard_logs
  learning_curve_plot: artifacts\ann-mnist-model\learning_curve_plot.png

mlflow_config:
  artifacts_dir: artifacts/mlflow-artifacts
  remote_server_uri: http://localhost:1234
  experiment_name: MNIST Digit model
  tensorflow_auto_log_every_n_iter: 2
  # 2: mlflow will log the training metrics (loss, accuracy, and validation loss etc.) every 2 epochs

model_learning_setup:
  model_type: "basic" # basic, batch_normalisation, batch_normalisation_without_bias
  optimizer: "adam" # "sgd", "adagrad", "adadelta", "rmsprop", "adam", "adamax"
  learning_rate: 0.001

  #1. SGD parameters
  momentum: 0.9 # 0.0

  #2. When Nesterov is true
  #  velocity = momentum * velocity - learning_rate * g
  #  w = w + momentum * velocity - learning_rate * g
  nesterov: True # False True

  #3. Ada grad parameters
  initial_accumulator_value: 0.1
  #  epsilon: 0.000001

  #4. Ada delta parameters
  rho: 0.95
  #  epsilon: 0.000001

  #5. RMS prop parameters
  #  momentum: 0.9 # 0.0
  #  rho: 0.95
  #  epsilon: 0.000001

  #6. Adam parameters
  beta_1: 0.9
  beta_2: 0.999
  epsilon: 0.000001 # 1e-08

  # binary_crossentropy, categorical_crossentropy, sparse_categorical_crossentropy
  loss_function: "sparse_categorical_crossentropy"
  metrics: [ "accuracy" ]

model_training_parameters:
  epochs: 20
  batch: 50
  configure_layers:
    hidden_layer_1_name: hidden_layer_1
    hidden_layer_1_activation: relu # sigmoid, tanh, relu, selu, elu, PReLU, LeakyReLU
    hidden_layer_1_number_of_neurons: 300
     # for sigmoid and tanh "glorot_uniform" "glorot_normal", xavier ~glorot
     # for ReLU "he_uniform" "he_normal"
    hidden_layer_1_kernel_initializer: glorot_normal
    hidden_layer_2_name: hidden_layer_2
    hidden_layer_2_activation: relu
    hidden_layer_2_number_of_neurons: 100
    hidden_layer_2_kernel_initializer: glorot_normal
    output_layer_name: output_layer
    output_layer_activation: softmax
    output_layer_number_of_neurons: 10

model_transfer_learning:
  model_to_load: artifacts/ann-mnist-model/Model_2021_06_07_20_19_03_.h5
  new_layer_activation: relu
  new_layer_name: hidden_layer_3
  new_output_layer_name: new_output_layer
  new_output_layer_activation: softmax # softmax binary

hyper_parameter_tuning:
  number_of_hidden_layers: [20, (40, 20), (45, 30, 15)]
  activation_functions: ['relu']# ['sigmoid', 'relu']
  batch_sizes: [50] # [128, 256]
  number_of_epochs: [2]
