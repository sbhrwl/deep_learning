{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"14.TypeOfOptimizers.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNgLLxB6aocIDvSREVEYUTj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lAkC5dQsZkGD"},"source":["# Gradient Descent\n","* Gradient Descent\n","\n","  <img src=\"https://render.githubusercontent.com/render/math?math=w = w - \\eta\\frac{\\partial y}{\\partial x}|_{w=w_{i}}\">"]},{"cell_type":"markdown","metadata":{"id":"q0-Xe4ejmGDD"},"source":["# SGD Momentum\n","   * SGD Momentum\n","     * SGD with Momentum is based on the basic concept of terminal velocity.\n","     * This alogrithm introduces a new term **momentum (m)** in the weight update formaula\n","     \n","          <img src=\"https://render.githubusercontent.com/render/math?math=w = w - m\">\n","          \n","          where m is given by below equation\n","          \n","          <img src=\"https://render.githubusercontent.com/render/math?math=m = \\beta m - \\eta\\frac{\\partial y}{\\partial x}\">\n","          \n","          **Beta** coefficient of momentum (Usually the value of Beta is 0.9)\n","     * Lets calculate **momentum** for 3rd weight          \n","          <img src=\"https://render.githubusercontent.com/render/math?math=m_{3} = \\eta \\left [ \\beta ^{2} \\frac{\\partial y}{\\partial x} |_{w =w_{0}} %2B \\beta \\frac{\\partial y}{\\partial x} |_{w =w_{1}} %2B \\frac{\\partial y}{\\partial x} |_{w =w_{2}}\\right ]\">\n","     * Weight update for W3 can be written as\n","     <img src=\"https://render.githubusercontent.com/render/math?math=w_{3} = w_{2} - m_{3}\">\n","\n","**Observation**: SGD with Momentum accumulates weights from previous step\n","* 81% of w=w0\n","* 90% of w=w1\n","* 100% of present Gradient w=w2\n","\n","Disadvantages:\n","* One extra parameter (Beta)\n","* It Oscillates when it reaches close to local/global minima because of **accumulation of past momentum**\n","\n","Advantages:\n","* Momentum helps in fast convergence\n","* Oscillation may help to come out of local minima and in turn look for **global minima**"]},{"cell_type":"markdown","metadata":{"id":"9xsFhhv6KmTe"},"source":["# Nesterov Accelerated Gradient\n","  * Nesterov Accelerated Gradient\n","    * Calculates Gradient slightly ahead in the direction of momentum\n","    * Weight updates are not done when it is close to the flat slope and hence prevents **Oscilations**\n","\n","    * Momentum Calulation\n","\n","      <img src=\"https://render.githubusercontent.com/render/math?math=m = \\beta_{m} %2B \\eta \\frac{\\partial C}{\\partial x}|_{w =w-\\beta_{m}}\">\n","    * Weight Update\n","\n","      <img src=\"https://render.githubusercontent.com/render/math?math=w = w - m\">\n","    * Gradient is calculated slightly ahead\n","      <img src='https://drive.google.com/uc?id=1T5vqplOgEVefBCfEEMyusqrsLYRGpLM5' width=500>\n","\n","      Observe below term\n","      <img src=\"https://render.githubusercontent.com/render/math?math=(w_{1}-\\beta_{m_{1}})\">\n","\n","      * w is +ve\n","      * Beta is +ve\n","      * m is +ve\n","      * So eventually we are **calculating Gradient** slightly ahead (Beta * m times ahead in the direction of **momentum**)\n","\n","      <img src='https://drive.google.com/uc?id=15YlT75scOMAQfLXahiuZN4KaZYl_MV8O' width=500>\n","\n","    * Lets calculate **momentum** for 3rd weight          \n","      <img src=\"https://render.githubusercontent.com/render/math?math=m_{3} = \\eta \\left [ \\beta ^{2} \\frac{\\partial y}{\\partial x} |_{w =w_{0}} %2B \\beta \\frac{\\partial y}{\\partial x} |_{w =(w_{1}-\\beta_{m_{1}})} %2B \\frac{\\partial y}{\\partial x} |_{w =(w_{2}-\\beta_{m_{2}})}\\right ]\">\n","    * Weight update for W3 can be written as\n","      <img src=\"https://render.githubusercontent.com/render/math?math=w_{3} = w_{2} - m_{3}\">\n","\n","**Observation**: SGD with Momentum along with Nesterov \n","* Accumulates weights from previous step\n","  * 81% of w=w0\n","  * 90% of w=w1\n","  * Present Gradient is **ZERO** when it is close to minima\n","* As we approach **minima**\n","  * Calculate Gradient, gradient will be zero as it is at minima\n","  * The weight update will not happen because gradient is zero or very small\n","  * This will prevent **Oscillations**\n","\n","    <img src='https://drive.google.com/uc?id=1mg_S_4AUpR5QI1mT5ADUXHkAgp5CydKR' width=600>\n","\n","Disadvantages:\n","* One extra parameter (Beta)\n","\n","Advantages\n","* Faster than SGD with momentum\n","* Lesser Oscillations\n","* Reaches close to minima\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JKtWOAApKsVC"},"source":["# AdaGrad\n","  * AdaGrad\n","    * AdaGrad solves **elongated bowl problem**\n","    * AdaGrad helps to correct the direction of graident initially\n","    \n","    <img src='https://drive.google.com/uc?id=1Tn_HcrfYLyiapLFA4IgY_rxE1q6eLpfT' width=800>\n","\n","    * **Decaying** Learning rate (fast Decay)\n","\n","      <img src=\"https://render.githubusercontent.com/render/math?math=w = w - \\frac{\\eta}{k}\\frac{\\partial y}{\\partial x}|_{w=w_{i}}\">\n","\n","Advantages\n","* Corrects its direction **Initially** unlike GD\n","* Less Tuning of Learning rate\n","\n","Disadvantages\n","* Stops early before reaching minima\n","* Takes longer time to converge due to decaying learning rate\n","\n","\n","* **Adagrad is NOT recommended**\n","* Another variation of Adagrad is AdaGradDelta"]},{"cell_type":"markdown","metadata":{"id":"73icDUGYWJLk"},"source":["# RMSPROP\n","  * RMSPROP \n","\n","    * Problems with GD\n","      * Traversing in flat region is very slow (Gradient is ZERO)\n","      * Direction correction is not possible\n","        * Adagrad addresses and solves the direction correction problem\n","      * Follows **Zig zag** pattern\n","        * Nesterov Accelerated Gradient (NAG) addresses the zig zag problem\n","\n","    * Problems with Adagrad\n","      * Stops early\n","\n","    * Zig Zag problem can be solved with inspiration from **Time Series** problems (Exponential decay)\n","\n","    * **Exponential decay** can be represented as follows\n","\n","      <img src=\"https://render.githubusercontent.com/render/math?math=P_{n} = \\beta P_{n-1} %2B (1-\\beta)q^{n}\">\n","\n","      * It only **accumulates**  \n","      <img src=\"https://render.githubusercontent.com/render/math?math=\\frac{1}{1-\\beta }\">\n","      samples\n","      * For Beta = 0.5 => **2 previous samples**\n","      * For Beta = 0.9\n","        <img src='https://drive.google.com/uc?id=1aUdz9QCHA-KuO6FjPqzWpSG8kv3DIsGT' width=800>\n","\n","    * RMS prop solves the problem of early stopping of Adagrad by accumulating gradients from **recent iterations** with the help of **exponential decay** (by introducing **Beta** term into Adagrad scaling factor)\n","      * Scaling factor, **s**\n","        \n","        <img src=\"https://render.githubusercontent.com/render/math?math=s = \\beta s %2B (1-\\beta)(\\frac{\\partial C}{\\partial x}|_{w})^{2}\">\n","      * Weight update together with **s** and Adjustment factor, **epsilon**\n","        \n","        <img src=\"https://render.githubusercontent.com/render/math?math=w = w - \\eta \\frac{\\frac{\\partial C}{\\partial x}|_{w}}{\\sqrt{s%2B\\epsilon }} \">\n","\n","    * Lets calculate **scaling factor** for 3rd weight          \n","      <img src=\"https://render.githubusercontent.com/render/math?math=s_{3} = \\beta s_{2} %2B (1-\\beta)(\\frac{\\partial C}{\\partial x}|_{w=w2})^{2}\">\n","    * Weight update for W3 can be written as\n","\n","      <img src=\"https://render.githubusercontent.com/render/math?math=w_{3} = w_{2} - \\eta \\frac{\\frac{\\partial C}{\\partial x}|_{w=w2}}{\\sqrt{s_{2}%2B\\epsilon }}\">\n","    * Comparison of Decay of Learing rate \n","      * With RMS prop, we multiply scaling factor with **Beta**\n","      * So, scaling factor is a **samller** value compared to Adagrad\n","      * This implies Learning rate is divided by a smaller value resulting in higher learing rate\n","      * Learning rate will not decay as in Adagrad\n","      * This results in solving early stopping problem of Adagrad\n","        <img src='https://drive.google.com/uc?id=1IUzSFnixfXpqsJ3JqRN-AeqGvZQdJRxa' width=1000>\n","      * **rho** ~ beta (0.9)"]},{"cell_type":"markdown","metadata":{"id":"PuYsuOmeau5p"},"source":["# ADAM\n","  * ADAM (Adaptive Moment Estimation)\n","    * Combines Idea of \n","      * Momentum Optimization: Exponential Decay of Gradients\n","\n","        <img src=\"https://render.githubusercontent.com/render/math?math=m = \\beta_{1} m - (1-\\beta_{1})\\frac{\\partial C}{\\partial x}|_{w}\">\n","      * RMS prop: Exponential Decay of **SQUARED** Gradients\n","        \n","        <img src=\"https://render.githubusercontent.com/render/math?math=s = \\beta_{2} s %2B (1-\\beta_{2})(\\frac{\\partial C}{\\partial x}|_{w})^{2}\">\n","    * Introduces **Bias correction** steps: inspired by Time series smoothing operations\n","      * Momentum Unit vector \n","\n","        <img src=\"https://render.githubusercontent.com/render/math?math=\\hat{m} = \\frac{m}{1-\\beta _{1}^{t}}\">\n","      * Scaling Unit vector\n","        <img src=\"https://render.githubusercontent.com/render/math?math=\\hat{s} = \\frac{s}{1-\\beta _{2}^{t}}\">\n","\n","    * Weight Update\n","\n","       <img src=\"https://render.githubusercontent.com/render/math?math=w = w - \\eta \\frac{\\hat{m}}{\\sqrt{\\hat{s}%2B\\epsilon }}\">\n","\n","**Adam optimizer incorporates all the best practices**"]},{"cell_type":"markdown","metadata":{"id":"KPP3UkJaNqEV"},"source":["## Initial Conditions\n","\n","<img src='https://drive.google.com/uc?id=1qL05zly9EGlXyMJJeYhf4zkQTOoZpCjM' width=800>"]},{"cell_type":"markdown","metadata":{"id":"dnthmZ6DMBzD"},"source":["## Step 1\n","\n","<img src='https://drive.google.com/uc?id=1rpi9mlhZeJLS8PfAom9ekNcPTTsUp0-Q' width=800>"]},{"cell_type":"markdown","metadata":{"id":"mRS3oavkNBQo"},"source":["## Step 2\n","\n","<img src='https://drive.google.com/uc?id=1Lr-cxnrDvVrkvIMLJDtnZ9BY92-x-qFX' width=800>"]},{"cell_type":"markdown","metadata":{"id":"-pkOFRZcQ8bz"},"source":["## Step 3\n","\n","<img src=\"https://render.githubusercontent.com/render/math?math=\\hat{m}_{3} = \\frac{m_{3}}{1-\\beta _{1}^{3}}\">"]},{"cell_type":"markdown","metadata":{"id":"sheT67Z-ND0F"},"source":["Advantages\n","* Addresses Bias issue\n","* Adam performs **adaptive** learning, so less tuning of Learing rate is required\n","\n","2 new parameters\n","* Beta 1\n","* Beta 2"]}]}